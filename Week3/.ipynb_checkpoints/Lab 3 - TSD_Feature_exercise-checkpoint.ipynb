{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering and Time Series Feature Generation\n",
    "\n",
    "### 08th Februry 2022 created by Yan Ge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Notebook, we will focus on:1) Time-series feature generation; 2) Feature generation with machine learning: case study of financial marketing with recommender systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources of this tutorial: 1). https://machinelearningmastery.com/basic-feature-engineering-time-series-data-python/; 2) https://github.com/Apress/hands-on-time-series-analylsis-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Feature Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we will use a daily stock price of Apple.Inc (one year: from 28/Dec/2020 to 27/Dec/2021), which is derived from Yahoo Finance. First, the time series is loaded as a Pandas Series. We then create a new Pandas DataFrame for the transformed dataset. Next, each column is added one at a time where month and day information is extracted from the time-stamp information for each observation in the series. Below is the Python code to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    month  day       price\n",
      "0      12   28  133.990005\n",
      "1      12   29  138.050003\n",
      "2      12   30  135.580002\n",
      "3      12   31  134.080002\n",
      "4       4    1  133.520004\n",
      "5       5    1  128.889999\n",
      "6       6    1  127.720001\n",
      "7       7    1  128.360001\n",
      "8       8    1  132.429993\n",
      "9      11    1  129.190002\n",
      "10     12    1  128.500000\n",
      "11      1   13  128.759995\n",
      "12      1   14  130.800003\n",
      "13      1   15  128.779999\n",
      "14      1   19  127.779999\n"
     ]
    }
   ],
   "source": [
    "# create date time features of a dataset\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "series = read_csv('data/AAPL.csv', header=0, index_col=0, parse_dates=True, squeeze=True)\n",
    "dataframe = DataFrame()\n",
    "dataframe['month'] = [series.index[i].month for i in range(len(series))]\n",
    "dataframe['day'] = [series.index[i].day for i in range(len(series))]\n",
    "dataframe['price'] = [series[i] for i in range(len(series))]\n",
    "print(dataframe.head(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lag feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lag features are the classical way that time series forecasting problems are transformed into supervised learning problems. The simplest approach is to predict the value at the next time (t+1) given the value at the previous time (t-1). The supervised learning problem with shifted values looks as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Value(t-1), Value(t+1)\n",
    "\n",
    "Value(t-1), Value(t+1)\n",
    "\n",
    "Value(t-1), Value(t+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Pandas library provides the shift() function to help create these shifted or lag features from a time series dataset. Note that: in this tutorial, shifting the dataset by 1 creates the t-1 column, adding a NaN (unknown) value for the first row. The time series dataset without a shift represents the t+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an example of creating a lag feature for our daily stock price dataset. The values are extracted from the loaded series and a shifted and unshifted list of these values is created. Each column is also named in the DataFrame for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "series = read_csv('data/AAPL.csv', header=0, index_col=0)\n",
    "temps = DataFrame(series.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shift operation with 1 lag\n",
    "lag_shift = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = concat([lag_shift, temps], axis=1)\n",
    "dataframe.columns = ['t-1', 't+1']\n",
    "print(dataframe.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that we would have to discard the first row to use the dataset to train a supervised learning model, as it does not contain enough data to work with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The addition of lag features is called the sliding window method, in this case with a window width of 1. It is as though we are sliding our focus along the time series for each observation with an interest in only what is within the window width."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can expand the window width and include more lagged features. For example, below is the above case modified to include the last 3 observed values to predict the value at the next time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "series = read_csv('data/AAPL.csv', header=0, index_col=0)\n",
    "temps = DataFrame(series.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shift operation with 1 lag, 2 lag and 3 lag\n",
    "lag_shift_one = \n",
    "lag_shift_two = \n",
    "lag_shift_three = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = concat([lag_shift_three, lag_shift_two, lag_shift_one, temps], axis=1)\n",
    "dataframe.columns = ['t-3', 't-2', 't-1', 't+1']\n",
    "print(dataframe.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, you can see that we must discard the first few rows that do not have enough data to train a supervised model. A difficulty with the sliding window approach is how large to make the window for your problem. Perhaps a good starting point is to perform a sensitivity analysis and try a suite of different window widths to in turn create a suite of different “views” of your dataset and see which results in better performing models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rolling Window Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A step beyond adding raw lagged values is to add a summary of the values at previous time steps. We can calculate summary statistics across the values in the sliding window and include these as features in our dataset. Perhaps the most useful is the mean of the previous few values, also called the rolling mean. For example, we can calculate the mean of the previous two values and use that to predict the next value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we need to do is shifted. Then the rolling dataset can be created and the mean values calculated on each window of two values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "series = read_csv('data/AAPL.csv', header=0, index_col=0)\n",
    "temps = DataFrame(series.values)\n",
    "shifted = temps.shift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shift rolling with window size 1\n",
    "\n",
    "\n",
    "# mean value in the window\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = concat([means, temps], axis=1)\n",
    "dataframe.columns = ['mean(t-2,t-1)', 't+1']\n",
    "print(dataframe.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is another example that shows a window width of 3 and a dataset comprised of more summary statistics, specifically the minimum, mean, and maximum value in the window. You can see in the code that we are explicitly specifying the sliding window width as a named variable. This lets us use it both in calculating the correct shift of the series and in specifying the width of the window to the rolling() function. In this case, the window width of 3 means we must shift the series forward by 2 time steps. This makes the first two rows NaN. Next, we need to calculate the window statistics with 3 values per window. It takes 3 rows before we even have enough data from the series in the window to start calculating statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          min        mean         max         t+1\n",
      "0         NaN         NaN         NaN  133.990005\n",
      "1         NaN         NaN         NaN  138.050003\n",
      "2         NaN         NaN         NaN  135.580002\n",
      "3         NaN         NaN         NaN  134.080002\n",
      "4  133.990005  135.873337  138.050003  133.520004\n"
     ]
    }
   ],
   "source": [
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "series = read_csv('data/AAPL.csv', header=0, index_col=0)\n",
    "temps = DataFrame(series.values)\n",
    "width = 3\n",
    "shifted = temps.shift(width - 1)\n",
    "window = shifted.rolling(window=width)\n",
    "dataframe = concat([window.min(), window.mean(), window.max(), temps], axis=1)\n",
    "dataframe.columns = ['min', 'mean', 'max', 't+1']\n",
    "print(dataframe.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expanding Window Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another type of window that may be useful includes all previous data in the series. This is called an expanding window and can help with keeping track of the bounds of observable data. Like the rolling() function on DataFrame, Pandas provides an expanding() function that collects sets of all prior values for each time step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an example of calculating the minimum, mean, and maximum values of the expanding window on the daily stock price dataset. Running the example prints the first 5 rows of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create expanding window features\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "series = read_csv('data/AAPL.csv', header=0, index_col=0)\n",
    "temps = DataFrame(series.values)\n",
    "window = temps.expanding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the min, mean and max value in the window\n",
    "win_min = \n",
    "win_mean = \n",
    "win_max = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = concat([win_min, win_mean,win_max, temps.shift(-1)], axis=1)\n",
    "dataframe.columns = ['min', 'mean', 'max', 't+1']\n",
    "print(dataframe.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Feature generation with machine learning: case study of financial marketing with recommender systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we use a real-world dataset from Amazon.com.Inc. In total, this dataset includes 478,235 users and 266,414 items. The data is from http://jmcauley.ucsd.edu/data/amazon/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A2EFCYXHNK06IS</td>\n",
       "      <td>5555991584</td>\n",
       "      <td>5.0</td>\n",
       "      <td>978480000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A1WR23ER5HMAA9</td>\n",
       "      <td>5555991584</td>\n",
       "      <td>5.0</td>\n",
       "      <td>953424000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A2IR4Q0GPAFJKW</td>\n",
       "      <td>5555991584</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1393545600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A2V0KUVAB9HSYO</td>\n",
       "      <td>5555991584</td>\n",
       "      <td>4.0</td>\n",
       "      <td>966124800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A1J0GL9HCA7ELW</td>\n",
       "      <td>5555991584</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1007683200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                0           1    2           3\n",
       "0  A2EFCYXHNK06IS  5555991584  5.0   978480000\n",
       "1  A1WR23ER5HMAA9  5555991584  5.0   953424000\n",
       "2  A2IR4Q0GPAFJKW  5555991584  4.0  1393545600\n",
       "3  A2V0KUVAB9HSYO  5555991584  4.0   966124800\n",
       "4  A1J0GL9HCA7ELW  5555991584  5.0  1007683200"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In this example, we only use 12,000 user-item pairs. But you can break this limitation by using the whole dataset.\n",
    "df=pd.read_csv('data/ratings_Digital_Music.csv',header=None, nrows=12000)\n",
    "df.head() #column order: user, items, ratings, timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12000, 4)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9843 unique users\n",
      "561 unique items\n",
      "5 unique ratings\n"
     ]
    }
   ],
   "source": [
    "n_users = df[0].unique().shape[0]\n",
    "n_items = df[1].unique().shape[0]\n",
    "n_rating = df[2].unique().shape[0]\n",
    "\n",
    "print ('%i unique users' %n_users)\n",
    "print ('%i unique items' %n_items)\n",
    "print ('%i unique ratings' %n_rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate user-item matrix\n",
    "ratings=df.pivot(index=0, columns=1, values=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill NaN values with 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a statistical analysis about the percetage of exsiting user-items ratings. From this statistics, we can see that \n",
    "# the vast majority of ratings are missing, which is our motivation to develop a recommender system to predict such missing values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This creats a validation dataset by selecting rows (user) that have 35 or more ratings, then randomly select 15 of those ratings\n",
    "#for validation set, but set those values to 0 in the training set.\n",
    "\n",
    "def train_test_split(ratings):\n",
    "    \n",
    "    validation = np.zeros(ratings.shape)\n",
    "    train = ratings.copy() #don't do train=ratings, otherwise, ratings becomes empty\n",
    "    \n",
    "    for user in np.arange(ratings.shape[0]):\n",
    "        if len(ratings[user,:].nonzero()[0])>=35:# 35 seems to be best, it depends on sparsity of your user-item matrix\n",
    "            val_ratings = np.random.choice(ratings[user, :].nonzero()[0], \n",
    "                                        size=15, #tweak this, 15 seems to be optimal\n",
    "                                        replace=False)\n",
    "            train[user, val_ratings] = 0\n",
    "            validation[user, val_ratings] = ratings[user, val_ratings]\n",
    "    print(validation.shape)\n",
    "    return train, validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split this dataset into train and test sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#P is latent user feature matrix\n",
    "#Q is latent item feature matrix\n",
    "# make a rating prediction given P and Q\n",
    "def prediction(P,Q):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmbda = 0.4 # Regularization parameter\n",
    "k = 3 #tweak this parameter\n",
    "m, n = train.shape  # Number of users and items\n",
    "n_epochs = 30  # Number of epochs\n",
    "alpha=0.01  # Learning rate\n",
    "\n",
    "P = 3 * np.random.rand(k,m) # Latent user feature matrix\n",
    "Q = 3 * np.random.rand(k,n) # Latent movie feature matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "def rmse(prediction, ground_truth):\n",
    "    prediction = prediction[ground_truth.nonzero()].flatten() \n",
    "    ground_truth = ground_truth[ground_truth.nonzero()].flatten()\n",
    "    return sqrt(mean_squared_error(prediction, ground_truth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_errors = []\n",
    "val_errors = []\n",
    "# Gradient descent for optimisation\n",
    "# Only consider items with ratings \n",
    "users,items = train.nonzero()      \n",
    "for epoch in range(n_epochs):\n",
    "    for u, i in zip(users,items):\n",
    "        e = train[u, i] - prediction(P[:,u],Q[:,i])  # Calculate error for gradient update\n",
    "        P[:,u] += alpha * ( e * Q[:,i] - lmbda * P[:,u]) # Update latent user feature matrix\n",
    "        Q[:,i] += alpha * ( e * P[:,u] - lmbda * Q[:,i])  # Update latent item feature matrix\n",
    "    \n",
    "    train_rmse = rmse(prediction(P,Q),train)\n",
    "    val_rmse = rmse(prediction(P,Q),val) \n",
    "    train_errors.append(train_rmse)\n",
    "    val_errors.append(val_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# visualise training process "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take a look at prediction vs. actual ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "SGD_prediction=prediction(P,Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimation= SGD_prediction[val.nonzero()]\n",
    "ground_truth = val[val.nonzero()]\n",
    "results=pd.DataFrame({'prediction':estimation, 'actual rating':ground_truth})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prediction</th>\n",
       "      <th>actual rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.969998</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.944109</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.108701</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.499998</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.885569</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   prediction  actual rating\n",
       "0    3.969998            5.0\n",
       "1    3.944109            5.0\n",
       "2    4.108701            2.0\n",
       "3    3.499998            2.0\n",
       "4    3.885569            5.0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
